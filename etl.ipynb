{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData, HiveContext\n",
    "from pyspark.sql.functions import udf, col, lit, expr, when, regexp_replace, floor, split, abs, concat, round\n",
    "from pyspark.sql.functions import year, month, quarter, dayofmonth, hour, weekofyear, date_format, date_add, mean\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "#####\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "#from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear, avg, monotonically_increasing_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guidance from Udacity Knowledge - https://knowledge.udacity.com/questions/911823\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create spark session, utilising saurfang's Spark SAS package\n",
    "    \n",
    "    OUTPUT: returns spark session to main\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_airport_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads airport data from local storage and creates:\n",
    "    - 'airports' table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepath to airport data file\n",
    "    airport_data = input_data\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"ident\", StringType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"elevation_ft\", DoubleType(), True),\n",
    "        StructField(\"continent\", StringType(), True),\n",
    "        StructField(\"iso_country\", StringType(), True),\n",
    "        StructField(\"iso_region\", StringType(), True),\n",
    "        StructField(\"municipality\", StringType(), True),\n",
    "        StructField(\"gps_code\", StringType(), True),\n",
    "        StructField(\"iata_code\", StringType(), True),\n",
    "        StructField(\"local_code\", StringType(), True),\n",
    "        StructField(\"coordinates\", StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # read airport data file\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"True\").load(airport_data)\n",
    "    \n",
    "    # Two-letter state codes are given in the format 'US-xx'\n",
    "    # Extract two-letter code from this column\n",
    "    df = df.withColumn('state_code', when(df.iso_region.startswith('US-'), regexp_replace(df.iso_region,'US-','')))\n",
    "    \n",
    "    # Latitude and longitude are given as a tuple\n",
    "    # Separate into distinct columns \n",
    "    df = df.withColumn('lat', split(df['coordinates'],',').getItem(0).cast('double'))\n",
    "    df = df.withColumn('long', split(df['coordinates'],',').getItem(1).cast('double'))\n",
    "    \n",
    "    # drop rows with nulls in latitude or longitude\n",
    "    df = df.na.drop(subset=['lat','long'])\n",
    "    \n",
    "    # Process latitude and longitude figures into the format given for temperature data\n",
    "    # E.g., from -56.52352135 to 56.52S\n",
    "    #\n",
    "    # Guidance taken from stack overflow user Daniel de Paula for regexp_replace use\n",
    "    # https://stackoverflow.com/questions/37038014/pyspark-replace-strings-in-spark-dataframe-column\n",
    "    #\n",
    "    # Guidance taken from Udacity GPT for startswith use\n",
    "    df = df.withColumn('lat_2dp', round(df.lat, 2))\n",
    "    df = df.withColumn('abs_lat', when(df.lat_2dp.startswith('-'), regexp_replace(df.lat_2dp,'-','')).otherwise(df.lat_2dp))\n",
    "    df = df.withColumn('latitude_direction', when(df.lat.startswith('-'), 'S').otherwise('N'))\n",
    "    df = df.withColumn('long_2dp', round(df.long, 2))\n",
    "    df = df.withColumn('abs_long', when(df.long_2dp.startswith('-'), regexp_replace(df.long_2dp,'-','')).otherwise(df.long_2dp))\n",
    "    df = df.withColumn('longitude_direction', when(df.long.startswith('-'), 'W').otherwise('E'))\n",
    "    \n",
    "    \n",
    "    # extract columns to create airports table\n",
    "    airport_table = df.select(col('municipality').alias('city'),\n",
    "                              'continent', \n",
    "                              'iso_country',\n",
    "                              'state_code',\n",
    "                              concat(df.abs_lat, df.latitude_direction).alias('latitude'),\n",
    "                              concat(df.abs_long, df.longitude_direction).alias('longitude')) \\\n",
    "                                .dropDuplicates()\n",
    "    \n",
    "    # Save table to parquet file\n",
    "    airport_table.write.mode('overwrite') \\\n",
    "        .parquet(output_data)\n",
    "        \n",
    "    # Create or replace airports view; print confirmation\n",
    "    print('Creating airports table')\n",
    "    airport_table.createOrReplaceTempView('airports')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return airport_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_demographics_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads demographics data from local storage and creates:\n",
    "    - demographics table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # get filepath to airport data file\n",
    "    demographics_data = input_data\n",
    "    \n",
    "    schema = StructType() \\\n",
    "        .add(\"City\", StringType(), True) \\\n",
    "        .add(\"State\", StringType(), True) \\\n",
    "        .add(\"Median Age\", DoubleType(), True) \\\n",
    "        .add(\"Male Population\", IntegerType(), True) \\\n",
    "        .add(\"Female Population\", IntegerType(), True) \\\n",
    "        .add(\"Total Population\", IntegerType(), True) \\\n",
    "        .add(\"Number of Veterans\", IntegerType(), True) \\\n",
    "        .add(\"Foreign-born\", IntegerType(), True) \\\n",
    "        .add(\"Average Household Size\", IntegerType(), True) \\\n",
    "        .add(\"State Code\", StringType(), True) \\\n",
    "        .add(\"Race\", StringType(), True) \\\n",
    "        .add(\"Count\", IntegerType(), True)\n",
    "    \n",
    "    # read demographics data file\n",
    "    df = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"True\").load(demographics_data)\n",
    "    \n",
    "    # extract columns to create demographics table\n",
    "    demographics_table = df.select(col('City').alias('city'),\n",
    "                                col('Median Age').alias('median_age'),\n",
    "                                col('Male Population').alias('male_population'),\n",
    "                                col('Female Population').alias('female_population'),\n",
    "                                col('Total Population').alias('total_population'),\n",
    "                                col('Number of Veterans').alias('veteran_population'),\n",
    "                                col('Foreign-born').alias('foreign_born_population'),\n",
    "                                col('Average Household Size').alias('avg_household_size'),\n",
    "                                col('State Code').alias('state_code')).dropDuplicates()\n",
    "    \n",
    "    # extract columns to create race table\n",
    "    race_table = df.select(col('City').alias('city'),\n",
    "                          col('State Code').alias('state_code'),\n",
    "                          col('Race').alias('race'),\n",
    "                          col('Count').alias('count')).dropDuplicates()\n",
    "    \n",
    "    # extract columns to create state_code table\n",
    "    state_code_table = df.select(col('State Code').alias('state_code'),\n",
    "                                col('State').alias('state')).dropDuplicates()\n",
    "    \n",
    "        \n",
    "    # Save tables to parquet files\n",
    "    demographics_table.write.mode('overwrite') \\\n",
    "        .parquet(output_data)\n",
    "    race_table.write.mode('overwrite') \\\n",
    "        .parquet(output_data)\n",
    "    state_code_table.write.mode('overwrite') \\\n",
    "        .parquet(output_data)\n",
    "    \n",
    "    # Create or replace demographics, race and state_code tables\n",
    "    print('Creating demographics table')\n",
    "    demographics_table.createOrReplaceTempView('demographics')\n",
    "    print('---Done---')\n",
    "    print('Creating race table')\n",
    "    race_table.createOrReplaceTempView('race_counts')\n",
    "    print('---Done---')\n",
    "    print('Creating state_code table')\n",
    "    state_code_table.createOrReplaceTempView('state_table')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return demographics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_temperature_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads temperature data from local storage and creates:\n",
    "    - temperature table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # get filepath to temperature data file\n",
    "    temperature_data = input_data\n",
    "    \n",
    "    schema = StructType() \\\n",
    "        .add(\"dt\", DateType(), True) \\\n",
    "        .add(\"AverageTemperature\", DoubleType(), True) \\\n",
    "        .add(\"AverageTemperatureUncertainty\", DoubleType(), True) \\\n",
    "        .add(\"City\", StringType(), True) \\\n",
    "        .add(\"Country\", StringType(), True) \\\n",
    "        .add(\"Latitude\", StringType(), True) \\\n",
    "        .add(\"Longitude\", StringType(), True)\n",
    "    \n",
    "    \n",
    "    # read demographics data file\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"True\").load(temperature_data)\n",
    "    \n",
    "    # keep only rows from the last 10 years\n",
    "    df = df.filter(col('dt')>lit('2003-09-01'))\n",
    "    \n",
    "    # extract columns to create temperature table\n",
    "    # extract month from the date column\n",
    "    # group by all columns while averaging the temperature\n",
    "    temp_avg_table = df.select('dt','AverageTemperature', 'City', 'Country', 'Latitude', 'Longitude') \\\n",
    "                            .groupby(month('dt').alias('month'),\n",
    "                                     col('City').alias('city'), \n",
    "                                     col('Country').alias('country'), \n",
    "                                     col('Latitude').alias('latitude'), \n",
    "                                     col('Longitude').alias('longitude')\n",
    "                                    ) \\\n",
    "                            .agg(mean('AverageTemperature').alias('avg_temp'))\n",
    "    \n",
    "    # Save table to parquet file\n",
    "    temp_avg_table.write.mode('overwrite').partitionBy('month') \\\n",
    "        .parquet(output_data)\n",
    "    \n",
    "    # Create or replace temperature table\n",
    "    print('Creating temperature table')\n",
    "    temp_avg_table.createOrReplaceTempView('temperature')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return temp_avg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads immigration data from local storage and creates:\n",
    "    - immigration table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # get filepath to immigration data file\n",
    "    immigration_data = input_data\n",
    "    \n",
    "    schema = StructType() \\\n",
    "        .add(\"cicid\", IntegerType(), True) \\\n",
    "        .add(\"i94yr\", IntegerType(), True) \\\n",
    "        .add(\"i94mon\", IntegerType(), True) \\\n",
    "        .add(\"i94cit\", IntegerType(), True) \\\n",
    "        .add(\"i94res\", IntegerType(), True) \\\n",
    "        .add(\"i94port\", StringType(), True) \\\n",
    "        .add(\"arrdate\", IntegerType(), True) \\\n",
    "        .add(\"i94mode\", IntegerType(), True) \\\n",
    "        .add(\"i94addr\", StringType(), True) \\\n",
    "        .add(\"depdate\", IntegerType(), True) \\\n",
    "        .add(\"i94bir\", IntegerType(), True) \\\n",
    "        .add(\"i94visa\", IntegerType(), True) \\\n",
    "        .add(\"count\", IntegerType(), True) \\\n",
    "        .add(\"dtadfile\", IntegerType(), True) \\\n",
    "        .add(\"visapost\", StringType(), True) \\\n",
    "        .add(\"occup\", StringType(), True) \\\n",
    "        .add(\"entdepa\", StringType(), True) \\\n",
    "        .add(\"entdepd\", StringType(), True) \\\n",
    "        .add(\"entdepu\", StringType(), True) \\\n",
    "        .add(\"matflag\", StringType(), True) \\\n",
    "        .add(\"biryear\", IntegerType(), True) \\\n",
    "        .add(\"dtaddto\", StringType(), True) \\\n",
    "        .add(\"gender\", StringType(), True) \\\n",
    "        .add(\"insnum\", DoubleType(), True) \\\n",
    "        .add(\"airline\", StringType(), True) \\\n",
    "        .add(\"admnum\", DoubleType(), True) \\\n",
    "        .add(\"fltno\", StringType(), True) \\\n",
    "        .add(\"visatype\", StringType(), True)\n",
    "\n",
    "    \n",
    "    # UDF to obtain arrival date by adding 'arrdate' number of days to 01/01/1960\n",
    "    ### From Udacity GPT ###\n",
    "    def add_days(days):\n",
    "        new_date = datetime.strptime('1960-01-01', '%Y-%m-%d') + timedelta(days=days)\n",
    "        return new_date.date()\n",
    "\n",
    "    add_days_udf = udf(add_days, DateType())\n",
    "    #######################\n",
    "    \n",
    "    \n",
    "   \n",
    "    # read immigration data file\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(input_data)\n",
    "    \n",
    "    # set arr_date to be integer format\n",
    "    df = df.withColumn('arrdate_int', floor(col('arrdate'))) \\\n",
    "           .withColumn('depdate_int', floor(col('depdate')))\n",
    "    # apply UDF\n",
    "    df = df.withColumn('arrival_date', add_days_udf(df.arrdate_int))\n",
    "    \n",
    "    # extract columns to create immigration table\n",
    "    immigration_table = df.select(col('cicid').alias('cicid').cast(\"integer\"),\n",
    "                                  col('i94yr').alias('I94_year').cast(\"integer\"),\n",
    "                                  col('i94mon').alias('I94_month').cast(\"integer\"),\n",
    "                                  col('i94cit').alias('cit_code').cast(\"integer\"),\n",
    "                                  col('i94res').alias('res_code').cast(\"integer\"),\n",
    "                                  col('i94port').alias('arrival_port'),\n",
    "                                  col('i94addr').alias('state_code'),\n",
    "                                  col('arrdate_int'),\n",
    "                                  col('arrival_date'),\n",
    "                                  col('depdate_int'),\n",
    "                                  col('i94bir').alias('age').cast(\"integer\"),\n",
    "                                  col('i94visa').alias('visa_code').cast(\"integer\"),\n",
    "                                  col('matflag').alias('match_flag'),\n",
    "                                  col('biryear').alias('birth_year').cast(\"integer\"),\n",
    "                                  col('dtaddto').alias('allowed_until'),\n",
    "                                  col('gender'),\n",
    "                                  col('airline')) \\\n",
    "                                    .withColumn('immigrant_id', monotonically_increasing_id()) \\\n",
    "                                    .dropDuplicates()\n",
    "\n",
    "    # extract columns to create arrival_dates table\n",
    "    date_table = df.select(col('arrdate_int'),\n",
    "                           col('arrival_date')) \\\n",
    "                            .withColumn('day', dayofmonth('arrival_date')) \\\n",
    "                            .withColumn('week', weekofyear('arrival_date')) \\\n",
    "                            .withColumn('month', month('arrival_date')) \\\n",
    "                            .withColumn('quarter', quarter('arrival_date')) \\\n",
    "                            .withColumn('year', year('arrival_date')) \\\n",
    "                            .dropDuplicates()\n",
    "    \n",
    "    # Save tables to parquet file\n",
    "    immigration_table.write.mode('overwrite').partitionBy('I94_year', 'I94_month') \\\n",
    "        .parquet(output_data)\n",
    "    date_table.write.mode('overwrite') \\\n",
    "        .parquet(output_data)\n",
    "    \n",
    "    # Create or replace immigration and dates tables\n",
    "    print('Creating immigration table')\n",
    "    immigration_table.createOrReplaceTempView('immigration')\n",
    "    print('---Done---')    \n",
    "    print('Creating date table')\n",
    "    date_table.createOrReplaceTempView('arrival_dates')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return immigration_table, date_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_port_data(spark):\n",
    "    \"\"\"\n",
    "    Gather port codes from local storage and creates:\n",
    "    - port_codes table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    file_path = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    \n",
    "    # load data and split rows into elements of a list\n",
    "    with open(file_path) as file:\n",
    "        i94_label_data = file.readlines()\n",
    "        i94_label_data = [item.split(\"\\n\")[0] for item in i94_label_data]\n",
    "    \n",
    "    # isolate port codes\n",
    "    portcode_start = [i for i, item in enumerate(i94_label_data) if 'I94PORT' in item][0]\n",
    "    portcode_end   = [i for i, item in enumerate(i94_label_data) if 'ARRDATE' in item][0]\n",
    "    port_code_list = i94_label_data[portcode_start+2:portcode_end-3]\n",
    "    \n",
    "    # extract port code, port name and state code from each element\n",
    "    port_codes = []\n",
    "    for item in port_code_list:\n",
    "        code_item = item.split('\\t=\\t')\n",
    "        code_item = [code_item[0].replace(\"'\",\"\"), code_item[1].replace(\"'\",\"\")]\n",
    "        code_item = [code_item[0].replace(\" \",\"\"), code_item[1].replace(\" \",\"\")]\n",
    "        code_item = [code_item[0]]+code_item[1].split(\",\")\n",
    "        \n",
    "        # some (~10) port names contain commas, which we ignore\n",
    "        if len(code_item) == 3:\n",
    "            port_codes = port_codes+[code_item]\n",
    "        \n",
    "    \n",
    "    df = pd.DataFrame(data=port_codes,columns=['port_code','city','state_code'])\n",
    "    \n",
    "    DF = spark.createDataFrame(df)\n",
    "    \n",
    "    # extract columns to create port_codes table\n",
    "    port_table = DF.select(col('port_code'),col('city'),col('state_code')) \\\n",
    "                        .dropDuplicates()\n",
    "\n",
    "    \n",
    "    # Create or replace port_codes table\n",
    "    print('Creating port code table')\n",
    "    port_table.createOrReplaceTempView('port_codes')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return port_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_cit_res_data(spark):\n",
    "    \"\"\"\n",
    "    Gathers citizen and resident codes from local storage and creates:\n",
    "    - cit_res table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    file_path = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    \n",
    "    # load data and split rows into elements of a list\n",
    "    with open(file_path) as file:\n",
    "        i94_label_data = file.readlines()\n",
    "        i94_label_data = [item.split(\"\\n\")[0] for item in i94_label_data]\n",
    "    \n",
    "    # isolate cit/res codes\n",
    "    citres_start = [i for i, item in enumerate(i94_label_data) if 'i94cntyl' in item][0]\n",
    "    citres_end   = [i for i, item in enumerate(i94_label_data) if 'I94PORT' in item][0]\n",
    "    citres_list = i94_label_data[citres_start+1:citres_end-2]\n",
    "        \n",
    "    # extract cit/res code and country from each element\n",
    "    citres_codes = []\n",
    "    for item in citres_list:\n",
    "        code_item = item.split('=')\n",
    "        code_item = [code_item[0].replace(\"'\",\"\"), code_item[1].replace(\"'\",\"\")]\n",
    "        code_item = [code_item[0].replace(\" \",\"\"), code_item[1].replace(\" \",\"\")]\n",
    "        \n",
    "        citres_codes = citres_codes+[code_item]\n",
    "    \n",
    "    df = pd.DataFrame(data=citres_codes,columns=['cit_res_code','country'])\n",
    "    \n",
    "    DF = spark.createDataFrame(df)\n",
    "    \n",
    "    # extract columns to create port_codes table\n",
    "    citres_table = DF.select(col('cit_res_code'),col('country')) \\\n",
    "                        .dropDuplicates()\n",
    "    \n",
    "    # Create or replace citres_codes table\n",
    "    print('Creating cit/res code table')\n",
    "    citres_table.createOrReplaceTempView('citres_codes')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return citres_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_visa_data(spark):\n",
    "    \"\"\"\n",
    "    Gathers visa codes from local storage and creates:\n",
    "    - visa_code table\n",
    "    and stores it locally in parquet format\n",
    "    \n",
    "    INPUTS: \n",
    "    spark - spark session\n",
    "    input_data - local location of dataset\n",
    "    output_data - local location for parquet table\n",
    "    \n",
    "    OUTPUTS:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    file_path = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "    \n",
    "    # load data and split rows into elements of a list\n",
    "    with open(file_path) as file:\n",
    "        i94_label_data = file.readlines()\n",
    "        i94_label_data = [item.split(\"\\n\")[0] for item in i94_label_data]\n",
    "    \n",
    "    # isolate visa codes\n",
    "    visa_start = [i for i, item in enumerate(i94_label_data) if 'I94VISA' in item][0]\n",
    "    visa_end   = [i for i, item in enumerate(i94_label_data) if '/* COUNT' in item][0]\n",
    "    visa_list = i94_label_data[visa_start+1:visa_end-3]\n",
    "    \n",
    "    # extract visa code and country from each element\n",
    "    visa_codes = []\n",
    "    for item in visa_list:\n",
    "        code_item = item.split(' = ')\n",
    "        code_item = [code_item[0].replace(\"'\",\"\"), code_item[1].replace(\"'\",\"\")]\n",
    "        code_item = [code_item[0].replace(\" \",\"\"), code_item[1].replace(\" \",\"\")]\n",
    "                \n",
    "        visa_codes = visa_codes+[code_item]\n",
    "        \n",
    "    df = pd.DataFrame(data=visa_codes,columns=['visa_code','visa_type'])\n",
    "    \n",
    "    DF = spark.createDataFrame(df)\n",
    "    \n",
    "    # extract columns to create port_codes table\n",
    "    visa_table = DF.select(col('visa_code'),col('visa_type')) \\\n",
    "                        .dropDuplicates()\n",
    "    \n",
    "    # Create or replace visa_codes table\n",
    "    print('Creating visa code table')\n",
    "    visa_table.createOrReplaceTempView('visa_codes')\n",
    "    print('---Done---')\n",
    "    \n",
    "    return visa_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_coord_check(temperature_table):\n",
    "    \"\"\"\n",
    "    This function completes a validity check on the temperature table\n",
    "    It checks whether the obtained values for latitude and longitude are valid\n",
    "    \n",
    "    INPUT: temperature_table (PySpark DF)\n",
    "    OUTPUT: None    \n",
    "    \"\"\"\n",
    "    \n",
    "    # latitude and longitude values should be in the range [-90, 90] and [-180, 180] respectively, \n",
    "    # or, as we have formatted these values, [90S, 90N] and [180W, 180E]. \n",
    "    # We should check that:\n",
    "    #     abs(latitude) <= 90\n",
    "    #     abs(longitude) <= 180\n",
    "    lat_long_check = temperature_table.withColumn('latitude_num', expr(\"substring(latitude, 1, length(latitude)-1)\")) \\\n",
    "                                      .withColumn('longitude_num', expr(\"substring(longitude, 1, length(longitude)-1)\"))\n",
    "    lat_long_check = lat_long_check.withColumn('latitude_num', lat_long_check.latitude_num.cast('double')) \\\n",
    "                                   .withColumn('longitude_num', lat_long_check.longitude_num.cast('double'))\n",
    "    max_lat = lat_long_check.agg({\"latitude_num\": \"max\"}).collect()[0]['max(latitude_num)']\n",
    "    max_long = lat_long_check.agg({\"longitude_num\": \"max\"}).collect()[0]['max(longitude_num)']\n",
    "    if max_lat > 90:\n",
    "        print('Invalid latitude in temperature data: {}'.format(max_lat))\n",
    "    if max_long > 180:\n",
    "        print('Invalid longitude in temperature data: {}'.format(max_long))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existence_check(table, data_file):\n",
    "    \"\"\"\n",
    "    This function completes a validity check on a table\n",
    "    It checks whether the constructed table is empty, indicating that an issue has occured in loading the data\n",
    "    \n",
    "    INPUT: PySpark DF\n",
    "    OUTPUT: None    \n",
    "    \"\"\"\n",
    "    \n",
    "    table_count = table.count()\n",
    "    if table_count == 0:\n",
    "        print(\"Table is empty - check source data: {}\",format(data_file))\n",
    "    else: \n",
    "        print(\"{} loaded successfully with {} rows\".format(data_file, table_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating port code table\n",
      "---Done---\n",
      "Creating cit/res code table\n",
      "---Done---\n",
      "Creating visa code table\n",
      "---Done---\n",
      "Creating airports table\n",
      "---Done---\n",
      "Creating demographics table\n",
      "---Done---\n",
      "Creating race table\n",
      "---Done---\n",
      "Creating state_code table\n",
      "---Done---\n",
      "Creating temperature table\n",
      "---Done---\n",
      "Creating immigration table\n",
      "---Done---\n",
      "Creating date table\n",
      "---Done---\n",
      "airport-codes_csv.csv loaded successfully with 53818 rows\n",
      "us-cities-demographics.csv loaded successfully with 596 rows\n",
      "../../data2/GlobalLandTemperaturesByCity.csv loaded successfully with 42120 rows\n",
      "../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat loaded successfully with 3096313 rows\n",
      "+--------------+---------+-----------+----------+--------+---------+\n",
      "|          city|continent|iso_country|state_code|latitude|longitude|\n",
      "+--------------+---------+-----------+----------+--------+---------+\n",
      "|      Tonasket|       NA|         US|        WA| 119.32S|   48.75E|\n",
      "|        Boston|       NA|         US|        MA|  71.13S|   42.37E|\n",
      "|  Green Valley|       NA|         US|        AZ| 111.12S|   31.91E|\n",
      "|     Kingsbury|       NA|         US|        NY|  73.48S|   43.34E|\n",
      "|Central Square|       NA|         US|        NY|  76.15S|   43.24E|\n",
      "|      Callaway|       NA|         US|        NE| 100.05S|   41.28E|\n",
      "|        Galena|       NA|         US|        IL|   90.4S|   42.41E|\n",
      "|     Kalispell|       NA|         US|        MT| 114.17S|   48.12E|\n",
      "|       Grafton|       NA|         US|        WV|  80.03S|   39.34E|\n",
      "|         Delta|       NA|         US|        CO| 108.13S|   38.73E|\n",
      "+--------------+---------+-----------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function in which:\n",
    "        - a spark session is created\n",
    "        - data source locations are defined\n",
    "        - datasets are processed into tables\n",
    "        - validity checks are completed\n",
    "        - output locations are defined\n",
    "    \"\"\"\n",
    "    \n",
    "    spark = create_spark_session()\n",
    "    airport_data = 'airport-codes_csv.csv'\n",
    "    aiport_output = 'airport_parquet.parquet'\n",
    "    demographics_data = 'us-cities-demographics.csv'\n",
    "    demographics_output = 'demographics_parquet.parquet'\n",
    "    temperature_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "    temperature_output = 'temperature_parquet.parquet'\n",
    "    immigration_data = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "    immigration_output = 'immigration_parquet.parquet'\n",
    "    \n",
    "    port_table = gather_port_data(spark)\n",
    "    citres_table = gather_cit_res_data(spark)\n",
    "    visa_table = gather_visa_data(spark)\n",
    "    \n",
    "    airport_table = process_airport_data(spark, airport_data, aiport_output)\n",
    "    demographics_table = process_demographics_data(spark, demographics_data, demographics_output)\n",
    "    temperature_table = process_temperature_data(spark, temperature_data, temperature_output)\n",
    "    immigration_table, arrival_date_table = process_immigration_data(spark, immigration_data, immigration_output)\n",
    "    \n",
    "    data_sets = [[airport_table, airport_data],\n",
    "                 [demographics_table, demographics_data],\n",
    "                 [temperature_table, temperature_data],\n",
    "                 [immigration_table, immigration_data]]\n",
    "    for data in data_sets:\n",
    "        existence_check(data[0],data[1])\n",
    "    valid_coord_check(temperature_table)\n",
    "    \n",
    "    \n",
    "    # Example queries\n",
    "    \n",
    "    #spark.sql(\"\"\"SELECT im.cicid, im.arrdate_int, im.arrival_port, dt.arrdate_int, dt.arrival_date, dt.month, temp.avg_temp FROM immigration im\n",
    "    #          LEFT JOIN arrival_dates dt\n",
    "    #          ON dt.arrdate_int = im.arrdate_int\n",
    "    #          LEFT JOIN temperature temp\n",
    "    #          ON temp.month = dt.month\n",
    "    #          LIMIT 10\n",
    "    #         \"\"\").show(10)\n",
    "    \n",
    "    spark.sql(\"\"\"SELECT * FROM airports LIMIT 10\"\"\").show(10)\n",
    "    \n",
    "    #spark.sql(\"\"\"SELECT * FROM temperature LIMIT 10\"\"\").show(10)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
